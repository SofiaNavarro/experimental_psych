{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sofia Navarro\n",
    "- Lab02 assignment\n",
    "- 9/9/2021\n",
    "\n",
    "\n",
    "## Challenge 1: Concordance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cds-\n",
      "[nltk_data]     au608644/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create austen variable\n",
    "austen = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# make all words lowecase\n",
    "austen = austen.lower()\n",
    "# replace every newline with nothing\n",
    "austen = austen.replace('\\n', ' ')\n",
    "# replace -- with space\n",
    "austen = austen.replace('--', ' ')\n",
    "# tokenize on whitespace\n",
    "austen = austen.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty concordace list\n",
    "concordance = []\n",
    "for i, val in enumerate(austen):\n",
    "    if val == 'away':\n",
    "        if austen[i-1] == 'passed':\n",
    "            concordance.append(str(' '.join(austen[i-5:i+5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: count all the words in every book in the gutenberg corpus, and find the longest book\n",
    "\n",
    "For this solution I wrote a quick tokenizer function which splits a text into any non-alphanumeric character using regex. I wanted to use this function for the first challenge as well, but it is not compatible with the later concordance for-loop.\n",
    "\n",
    "\n",
    "This solution creates a .csv file with all the book titles, number of unique words and number of total words in an output folder. However, I could not get the zip function to word in this loop, so I could not find a smart way of spitting out which was the longest book (even though a quick glance at the list tells me it's the Bible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer function\n",
    "def tokenize(input_string):\n",
    "    # Split on any non-alphanumeric character:\n",
    "    tokenizer = re.compile(r\"\\W+\") # what comes after r should be parsed as regex\n",
    "    # Tokenize\n",
    "    token_list = tokenizer.split(input_string)\n",
    "    # Return token_list\n",
    "    return token_list\n",
    "\n",
    "# create outpath to which the .csv file will be saved\n",
    "outpath = os.path.join(\"output\", \"word_count.csv\")\n",
    "\n",
    "# create titles for the .csv file\n",
    "table_titles = \" \".join([\"book_title\", \"unique_words\", \"total_words\"])\n",
    "\n",
    "# save the titles to the outpath\n",
    "with open(outpath, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(table_titles + \"\\n\")\n",
    "    \n",
    "# create variable containing the titles of the books\n",
    "books = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "# a for-loop that goes through every book in the corpus\n",
    "for book in books:\n",
    "    # read each book in the corpus\n",
    "    each_book = nltk.corpus.gutenberg.raw(book)\n",
    "    # tokenize on whitespace\n",
    "    split_book = tokenize(each_book)\n",
    "    # Convert all elements in split_book to a set, so each element only occurs once (to get number of unique words)\n",
    "    unique_words = set(split_book)\n",
    "    # get total amount of unique words and total amount of words \n",
    "    # make a list of the book titles\n",
    "    book_titles = [books]\n",
    "    # make a list of total amount of words\n",
    "    amount_words = [book, str(len(unique_words)), str(len(split_book))]\n",
    "    # append book title + unique words + total words in a string\n",
    "    save_file = \" \".join(amount_words)\n",
    "    # save the whole list in the outpath\n",
    "    with open(outpath, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(save_file + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jane Austen's 'Emma'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the ten most common words in Jane Austen's 'Emma' are [('to', 5041), ('the', 4813), ('and', 4305), ('of', 4187), ('a', 2958), ('I', 2602), ('was', 2297), ('in', 2034), ('not', 2013), ('her', 1915)]\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Jane Austen's book in\n",
    "book = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "# make all words lowercase, remove newlines and tokenize on whitespace\n",
    "words = book.lower()\n",
    "words = book.replace(\"\\n\", \" \")\n",
    "words = book.split()\n",
    "\n",
    "# create most frequent words variable\n",
    "word_frequency = nltk.FreqDist(words)\n",
    "\n",
    "f\"the ten most common words in Jane Austen's 'Emma' are {word_frequency.most_common(10)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Herman Melvile's 'Moby Dick'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the ten most common words in Herman Melville's 'Moby Dick' are [('the', 13604), ('of', 6475), ('and', 5881), ('a', 4472), ('to', 4439), ('in', 3824), ('that', 2680), ('his', 2415), ('I', 1724), ('with', 1645)]\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Herman Melville's book in\n",
    "book2 = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "# make all words lowercase, remove newlines and tokenize on whitespace\n",
    "words2 = book2.lower()\n",
    "words2 = book2.replace(\"\\n\", \" \")\n",
    "words2 = book2.split()\n",
    "\n",
    "# create most frequent words variable\n",
    "word_frequency2 = nltk.FreqDist(words2)\n",
    "\n",
    "f\"the ten most common words in Herman Melville's 'Moby Dick' are {word_frequency2.most_common(10)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: remove stopwords\n",
    "\n",
    "For this solution I will be using the built-in nltk stopwords library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jane Austen's 'Emma'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most frequent words of the filtered Jane Austen book are [('I', 2602), ('Mr.', 1097), ('would', 792), ('could', 792), ('Mrs.', 675), ('Miss', 561), ('must', 542), ('She', 492), ('Emma', 481), ('much', 424)]\n"
     ]
    }
   ],
   "source": [
    "# make a list of stopwords using the built-in nltk library\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# load Jane Austen's book\n",
    "book = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "tokens = book.lower()\n",
    "tokens = book.replace(\"\\n\", \" \")\n",
    "tokens = book.split()\n",
    "\n",
    "# filter out the stopwords from Austen's book\n",
    "filter_stopwords = [word for word in tokens if not word.lower in stopwords]\n",
    "filter_stopwords = []\n",
    "\n",
    "# loop through the words in the book and apply filter\n",
    "for w in tokens:\n",
    "    if w not in stopwords:\n",
    "        filter_stopwords.append(w)\n",
    "\n",
    "au_filtered_freq = nltk.FreqDist(filter_stopwords) \n",
    "        \n",
    "print(f\"The 10 most frequent words of the filtered Jane Austen book are {au_filtered_freq.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Herman Melville's 'Moby Dick'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most frequent words of the filtered Herman Melville book are [('I', 1724), ('one', 750), ('But', 638), ('like', 544), ('The', 531), ('upon', 531), ('old', 412), ('would', 406), ('whale', 392), ('And', 321)]\n"
     ]
    }
   ],
   "source": [
    "# make a list of stopwords using the built-in nltk library\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# load Herman Melville's book\n",
    "book2 = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "tokens2 = book2.lower()\n",
    "tokens2 = book2.replace(\"\\n\", \" \")\n",
    "tokens2 = book2.replace(\"\\r\", \" \")\n",
    "tokens2 = book2.split()\n",
    "\n",
    "# filter out the stopwords from Melville's book\n",
    "filter_stopwords2 = [word for word in tokens2 if not word.lower in stopwords]\n",
    "filter_stopwords2 = []\n",
    "\n",
    "# loop through the words in the book and apply filter\n",
    "for w in tokens2:\n",
    "    if w not in stopwords:\n",
    "        filter_stopwords2.append(w)\n",
    "\n",
    "au_filtered_freq2 = nltk.FreqDist(filter_stopwords2) \n",
    "        \n",
    "print(f\"The 10 most frequent words of the filtered Herman Melville book are {au_filtered_freq2.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is not working with nltk's built-in stopword list, since \"I\", \"the\", \"and\" etc. are still being counted, even though they should have been filtered out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
